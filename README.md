# Data Pipeline com PySpark: Vendas e Cota√ß√µes

Este projeto tem como objetivo aplicar t√©cnicas de **engenharia e an√°lise de dados com PySpark**.  
A proposta consiste em processar dados de vendas e cota√ß√µes a partir de arquivos CSV brutos, realizando:

- Limpeza e padroniza√ß√£o de informa√ß√µes (datas, acentos, nomes de cidades);
- Cria√ß√£o de DataFrames estruturados;
- Unifica√ß√£o de tabelas de vendas e cota√ß√µes;
- C√°lculo de m√©tricas de neg√≥cio, como total de vendas, itens cotados e taxa de convers√£o por local;
- Armazenamento otimizado dos dados em formato Delta.

O projeto demonstra habilidades em **processamento de dados em larga escala, transforma√ß√£o, an√°lise e persist√™ncia** usando PySpark.

---

## üîß Tecnologias Utilizadas

- Python 3.x  
- Apache Spark (PySpark)  
- Delta Lake  
- Spark SQL  

---

## üìÇ Estrutura do Projeto

- **vendas.csv** ‚Üí Arquivo bruto de vendas.  
- **cotacoes.csv** ‚Üí Arquivo bruto de cota√ß√µes.  
- **pipeline.py** ‚Üí Script PySpark que realiza todo o processamento, unifica√ß√£o, an√°lises e salvamento dos dados.  

> Obs: Os dados originais **n√£o est√£o inclusos** no reposit√≥rio.  
> Caso queira executar o projeto, crie arquivos `vendas.csv` e `cotacoes.csv` seguindo os schemas abaixo.

---

## üìä Funcionalidades do Projeto

- **Limpeza e padroniza√ß√£o de dados**  
  - Remo√ß√£o de acentos, espa√ßos e inconsist√™ncias.  
  - Corre√ß√£o de nomes de cidades e datas.  

- **Transforma√ß√£o**  
  - Cria√ß√£o de DataFrames estruturados a partir de arquivos CSV brutos.  
  - Convers√£o de colunas para tipos adequados (`int`, `double`, `date`).  

- **Unifica√ß√£o**  
  - Jun√ß√£o das tabelas de vendas e cota√ß√µes com base em chaves comuns (`revendedor_id`, `local`, `tipo_maquina`).  

- **An√°lises realizadas**  
  - Total de vendas por local.  
  - N√∫mero de itens cotados por local.  
  - Taxa de convers√£o (cota√ß√µes ‚Üí vendas) por local.  

- **Persist√™ncia**  
  - Armazenamento otimizado em formato Delta Lake.  
